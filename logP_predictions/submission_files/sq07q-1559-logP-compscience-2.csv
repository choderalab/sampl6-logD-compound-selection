# PARTITION COEFFICIENT PREDICTIONS
Predictions:
SM02,4.18,0.13,0.29
SM04,3.70,0.14,0.29
SM07,3.06,0.07,0.29
SM08,2.41,0.10,0.29
SM09,3.36,0.14,0.29
SM11,1.57,0.09,0.29
SM12,3.97,0.09,0.29
SM13,3.43,0.07,0.29
SM14,2.31,0.10,0.29
SM15,2.32,0.10,0.29
SM16,3.28,0.14,0.29

# NAME SECTION
Name:
Local XGBoost-Based QSPR LogP Predictor

# SOFTWARE SECTION
Software:
Python 3.6.8
RDKit  2018.09.1
XGBoost 0.81

# METHOD CATEGORY SECTION
Category:
Empirical

# METHOD DESCRIPTION SECTION
Method:
We have used EPI Kow dataset(from EPA's OPERA toolkit) as training set for supervised machine learning model using experimentally measured logP as label. Training set was preproccesed in generally accepted QSAR-ready matter (desalting, metals disconnected, NO2 groups standardized etc). Compounds with Tanimoto similary on ECFP4 fingerprints lower than 0.16 to target sampl6 challenge compounds were discarded resulting in ~659 compounds in the training set.
Featurization was done using RDKit combining several type of fingerprints (ECFP4 + Avalon1024 + MACCS keys) and 199 available descriptor in RDKit(MolWt, Chi's, Kappa, BalabanJ etc.)
XGBoost library implementation of extreme gradient boosting trees-based method was used as ML algorithm. 10-fold cross validation was used to evaluate MAE (used as predicted uncertainty) of logP prediction.
Local in the name of the model mean that it was trained on the small set of chemical structures with high similarity to target challenge compounds (up to 0.4) and differs from the submission 1 which is trained on the large diverse dataset.

